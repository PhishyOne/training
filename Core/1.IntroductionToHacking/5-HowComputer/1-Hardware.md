We have learned about learning and the hacker mindset, and have begun to learn about Linux and set up our new virtual machine. You finished Bandit, and should feel fairly comfortable reading something technical, taking that new information, and then immediately applying it to solve a problem. But now we are at the most important and difficult part of the course, how computers work. Without an in depth understanding of how computers work from a fundamental level, you will not have the low level knowledge required to understand things as they occur. As you read a news article, watch a presentation, or find a new CTF problem, you need the ability to recognize patterns and have the baseline knowledge required to immediately know what is going on.

That doesn't mean you will understand everything, the point is to build your baseline so that you can constantly have a wide breadth of knowledge to draw from. Depth of knowledge will be learned in follow on courses and your own personal path. With a strong base, you will minimize the time you spend backtracking and relearning things, as well as minimize the time required to learn new concepts. Right now, this is your time to rabbit hole and spend some time so you know enough information to move forward and not worry about having to learn fundamental concepts as you go.

### History of Computers

   * Read this: <https://www.explainthatstuff.com/historyofcomputers.html>
      * Don't be afraid to rabbit hole to make sure you understand what is going on.
   1. How did mechanical computers work?
   2. What is a vacuum tube and how was it important to the development of computing?
   3. What is a transistor and how did it contribute?
   4. What is an integrated circuit?
   5. What is a semi-conductor?
   5. Discuss the rise of the personal computer.

## Electricity
* It is always useful to know more about electricity <https://learn.sparkfun.com/tutorials/what-is-electricity>
* Read this: <https://www.explainthatstuff.com/electronics.html>
* Read this: <https://www.explainthatstuff.com/howtransistorswork.html>
* <https://learn.sparkfun.com/tutorials/analog-vs-digital>
* <https://learn.sparkfun.com/tutorials/logic-levels>

  1. How do transistors work?
  2. What converts a signal from analog to digital, and vice versa? How does it work?
  3. What is a logic level? How does that work in modern computers?


## Storage Density

   * <https://www.dataversity.net/brief-history-data-storage/>
   * <https://www.computerhope.com/jargon/p/punccard.htm>
   * <https://www.reddit.com/r/explainlikeimfive/comments/1t4r0g/eli5_how_did_punch_cards_work_specifically_what/>
   * <https://www.explainthatstuff.com/harddrive.html>
   * <https://www.explainthatstuff.com/flashmemory.html>
   * You will have to Google for most of these questions.

   1. Write a brief bit about the importance of storage density.
   2. How did Punch Cards work?
   3. How did magnetic tape work?
   4. How do floppy discs work?
   5. How do CDs work?
   6. How does flash memory work?
   7. How do USBs work?
   8. How do SSDs work?



## Telecommunication Basics
      * Read all of this.
      * <https://en.wikipedia.org/wiki/Telecommunication>
      * <https://en.wikipedia.org/wiki/Naval_flag_signalling>
      * <https://en.wikipedia.org/wiki/Flag_semaphore>
      * <https://en.wikipedia.org/wiki/Morse_code>
      * <https://en.wikipedia.org/wiki/ARPANET>
      * <https://en.wikipedia.org/wiki/Packet_switching>
      * If you haven't noticed yet, the military cares a lot about sending information.

      1. Write a brief bit about the importance of bits per second.
      2. Write a couple sentences about what ARPANET was.


## Representing Data

   * How do these ones and zeros work <https://learn.sparkfun.com/tutorials/binary>
   * Also hey, what is hex? <https://learn.sparkfun.com/tutorials/hexadecimal>

   1. Download a Hex Editor!
   2. Look at this file and view it in the Hex Editor.

   * Also, hey, what is ascii? This is ascii. <https://learn.sparkfun.com/tutorials/ascii>

   1. Do this <https://code.tutsplus.com/articles/number-systems-an-introduction-to-binary-hexadecimal-and-more--active-10848. Submit a screenshot showing your completion.>


   0. How many bits are used for each character in ASCII?
   1. "01101000 01100101 01101100 01101100 01101111 00100001" is in binary. Convert it back to ASCII. Do it by hand using the chart.
   2. Alright. Now. Convert 17 in Ascii to Binary and Hex. Do it by hand.
   3. Convert "Go Navy" to Octal, Hex, and Binary. Yes. By hand. Use Google to figure out how.
   4. What does "c2l4dHlmb3Vy" translate to from Base64?

* Read this <https://www.usna.edu/Users/cs/wcbrown/courses/si110AY13S/lec/l01/lec.html>
* Do the HW <https://www.usna.edu/Users/cs/wcbrown/courses/si110AY13S/lec/l01/hw/hw.pdf>
1. Submit pictures of the HW.

* Read this <https://www.usna.edu/Users/cs/wcbrown/courses/si110AY13S/lec/l02/lec.html>
* Do the HW <https://www.usna.edu/Users/cs/wcbrown/courses/si110AY13S/lec/l02/hw/hw.pdf>
1. Submit pictures of the HW.

* It is annoying to do that by hand, but it does help. Here is a tool so that you never have to do that again. <https://gchq.github.io/CyberChef/>. Fun fact, it is released by GCHQ, Britain's version of the NSA. Great tool and very useful. There are about a thousand uses for this thing, you can find a bunch on GitHub.


## Digital Logic Basics
* <https://learn.sparkfun.com/tutorials/binary>
* Digital logic: <https://learn.sparkfun.com/tutorials/digital-logic>
* Read this one too:  <https://web.archive.org/web/20061008134026/>
* <http://www.facstaff.bucknell.edu:80/mastascu/eLessonsHTML/Logic/Logic1.html>
* And this: <https://www.explainthatstuff.com/logicgates.html>

 1. How long is a bit? A nibble? A byte? A megabyte?
 2. What is a bitwise operator? How do they work?
 3. What is a bit shift? How does that work?
 4. Explain how digital logic is an essential part of computing.
 5. What are logic gates?
 6. How do logic gates relate to bitwise operators?

## Processing
   * <https://www.zzoomit.com/evolution-of-computer-processors/>
   * <https://www.techjunkie.com/a-cpu-history/>
   * <https://computer.howstuffworks.com/moores-law.htm>
   * <https://www.geeksforgeeks.org/difference-32-bit-64-bit-operating-systems/>
   * <https://computer.howstuffworks.com/microprocessor.htm>
   * <https://www.geeksforgeeks.org/difference-between-cpu-and-gpu/>

1. Explain the importance of processor density.
2. What does a 16 bit processor meaning? A 32 bit processor? A 64 bit processor?
3. What is the difference between a 32 bit and 64 bit operating system? Can a 64 bit OS run on a 32 bit OS? How about vice versa?
4. What version of Windows are you running? <https://support.microsoft.com/en-us/help/827218/how-to-determine-whether-a-computer-is-running-a-32-bit-version-or-64. This will primarily be relevant when installing programs.>
5. How does a modern processor work?  
6. What is a CPU?
7. What is a GPU? What makes it different from a CPU? What benefits does it have?


# Hardware Basics

* Using Google, explain all these things. Put it in a text file and submit all at once.

1. Motherboard
2. PCI Slot
3. CPU
4. Why is a fan needed?
5. RAM
6. Hard Drive
7. Optical Hard Drive
8. Solid State Hard Drive     
9. Graphics Card


# Levels of Code
## Machine Code and Assembly Code

In order to be executed on a CPU, code must be written in a way that is understood by the CPU. This is called 'Machine Code' and consists of 1s and 0s. As a note, there are various families of CPU architectures which require different machine codes to operate properly. Humans don't like 1s and 0s as much as computers so we prefer to abstract ourselves away from the bits as far as possible. The first step of this is something called an "Assembler". You can generally think of an assembler as a piece of code written for a specific type of CPU that does a 1 for 1 exact translation of code into binary that can be executed.

I like this video: <https://www.youtube.com/watch?v=wA2oMRmbrfo>

Assemblers do a 1 for 1 translation, but what if we are looking for something that can optimize or simplify the work needed to be done by the programmer? In that case we would need something called a compiler. A compiler is a computer program that translates computer code written in one programming language (the source language) into another programming language (the target language). This allows the programmer to save huge amounts of time and spend more time doing and less time working.

Hey, read this about our namesake Grace Hopper and the first compiler: <https://history-computer.com/ModernComputer/Software/FirstCompiler.html>

Watch this: <https://www.youtube.com/watch?v=IhC7sdYe-Jg>

Some day, you will probably write a compiler so all of the things you just learned about makes more sense to you. Or you can not. I did it once. Didn't get as much out of it as people told me I would.

What we went over was just the absolute basics of computer hardware. I did not spend any time on Machine Code and Assembly, and you will get destroyed by it the first time you see it. My recommendation, avoid anything involving machine or assembly code until after you have spent some dedicated time on the subject. This means, don't try to jump into any Reverse Engineering problems, for now.

## Memory
Memory... where do we start. Few things are harder to wrap our heads around how it all works. This course does not pretend it will teach you everything you will need to know about memory, this is just giving you the tools you need to continue learning. With that said, pay attention here, and refer back to help understand things.

* <https://www.tutorialspoint.com/computer_fundamentals/computer_memory.htm>

1. What makes up primary memory and what is it used for?
2. What is secondary memory and what is it used for?
3. What do we need a cache for?

* <https://www.tutorialspoint.com/computer_fundamentals/computer_ram.htm>

1. What does RAM stand for?
2. What makes RAM volatile?
3. What happens when a computer is turned off?
4. Read about a cold boot attack... how does that work? <https://arstechnica.com/gadgets/2018/09/cold-boot-attacks-given-new-life-with-firmware-attack/>

Plenty of cyber security purists will lose their mind if I do not acknowledge the existence of virtual memory and "the stack" during the section regarding memory. "The stack" is the region of memory, in RAM, where data is added and removed by processes. That is all you are going to learn in this course. All binary exploitation is dependent on in depth knowledge of "the stack"... and you won't be doing any binary exploitation during this course. Don't worry about it, soon you'll be "Smashing the Stack for Fun and Profit", but for now, you need to learn what an operating system does for you.

* Read this... I probably need a better resource but nobody makes short guides to this... because it is complicated as hell. <https://www.allaboutcircuits.com/technical-articles/what-is-virtual-memory/>
* Read this down to the Examples: <https://www.geeksforgeeks.org/memory-layout-of-c-program/>

1. What is the stack?
2. What is the heap?
3. What does memory allocation mean?
4. Where is memory allocated?
5. What does memory de-allocation mean?
6. What happens when memory is de-allocated?
